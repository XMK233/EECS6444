{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "\"cannot open file 'eclipse-metrics-files-2.0.csv': No such file or directory\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.table(\"eclipse-metrics-files-2.0.csv\", header = TRUE, sep = \";\")",
      "2. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "files_20 <- read.table(\"eclipse-metrics-files-2.0.csv\", header=T, sep=\";\")\n",
    "files_21 <- read.table(\"eclipse-metrics-files-2.1.csv\", header=T, sep=\";\")\n",
    "files_30 <- read.table(\"eclipse-metrics-files-3.0.csv\", header=T, sep=\";\")\n",
    "\n",
    "packages_20 <- read.table(\"eclipse-metrics-packages-2.0.csv\", header=T, sep=\";\")\n",
    "packages_21 <- read.table(\"eclipse-metrics-packages-2.1.csv\", header=T, sep=\";\")\n",
    "packages_30 <- read.table(\"eclipse-metrics-packages-3.0.csv\", header=T, sep=\";\")\n",
    "\n",
    "# setwd()\n",
    "\n",
    "\n",
    "nrow(files_20)\n",
    "nrow(files_21)\n",
    "nrow(files_30)\n",
    "nrow(packages_20)\n",
    "nrow(packages_21)\n",
    "nrow(packages_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mar=c(5, 5, 2, 1) + 0.1)\n",
    "hist(packages_30$post, freq=T, breaks=100, xlim=c(0,70), axes=F, main=\"\", xlab=\"Number of Post-Release Defects (per Package)\", ylab=\"Percentage\", col=\"darkgray\")\n",
    "axis(1)\n",
    "\n",
    "## this line, can draw the 0, 10%, 20% etc.. E.g., the 66.1 means the 661 (total number of lines) / 100 + 10. \n",
    "axis(2, at=c(0,66.1,66.1*2,66.1*3,66.1*4,66.1*5,66.1*6), labels=c(\"0%\",\"10%\",\"20%\",\"30%\",\"40%\",\"50%\",\"60%\"), las=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.p <- rep (-1, 33)\n",
    "post.p <- rep (-1, 33)\n",
    "for (i in 3:35) {\n",
    "    pre.p[i-2] <- cor.test(files_30[,i], files_30$pre, method=\"spearman\", exact=FALSE)$p.value\n",
    "    post.p[i-2] <- cor.test(files_30[,i], files_30$post, method=\"spearman\", exact=FALSE)$p.value\n",
    "}\n",
    "\n",
    "cbind(cor(files_30[,3:35], files_30$pre, method=\"spearman\"), cor(files_30[,3:35], files_30$post, method=\"spearman\"), (pre.p<0.01), (post.p<0.01))\n",
    "\n",
    "pre.p <- rep (-1, 42)\n",
    "post.p <- rep (-1, 42)\n",
    "for (i in 3:44) {\n",
    "    pre.p[i-2] <- cor.test(packages_30[,i], packages_30 $pre, method=\"spearman\", exact=FALSE)$p.value\n",
    "    post.p[i-2] <- cor.test(packages_30[,i], packages_30 $post, method=\"spearman\", exact=FALSE)$p.value\n",
    "}\n",
    "\n",
    "cbind(cor(packages_30[,3:44], packages_30$pre, method=\"spearman\"), cor(packages_30[,3:44], packages_30$post, method=\"spearman\"), (pre.p<0.01), (post.p<0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classification <- function (train, test) \n",
    "{\n",
    "    model.glm <- glm((post>0) ~ pre + ACD + FOUT_avg + FOUT_max + FOUT_sum + MLOC_avg + MLOC_max + MLOC_sum + NBD_avg + NBD_max + NBD_sum + NOF_avg + NOF_max + NOF_sum + NOI + NOM_avg + NOM_max + NOM_sum + NOT + NSF_avg + NSF_max + NSF_sum + NSM_avg + NSM_max + NSM_sum + PAR_avg + PAR_max + PAR_sum + + + TLOC + VG_avg + VG_max + VG_sum, data=train, family = \"binomial\")\n",
    "    test.prob <- predict(model.glm, test, type=\"response\")\n",
    "    test.pred <- test.prob>=0.50\n",
    "\n",
    "    outcome <- table(factor(test$post>0, levels=c(F,T)), factor(test.pred, levels=c(F,T)))\n",
    "    TN <- outcome[1,1]\n",
    "    FN <- outcome[2,1]\n",
    "    FP <- outcome[1,2]\n",
    "    TP <- outcome[2,2]\n",
    "    precision <- if (TP + FP ==0) { 1 } else { TP / (TP + FP) }\n",
    "    recall <- TP / (TP + FN)\n",
    "    accuracy <- (TP + TN) / (TN + FN + FP + TP)\n",
    "    defects <- (TP + FN) / (TN + FN + FP + TP)\n",
    "    return (c(defects, precision, recall, accuracy))\n",
    "}\n",
    "\n",
    "test_classification_pkg <- function (train, test) \n",
    "{\n",
    "    model.glm <- glm((post>0) ~ pre + ACD_avg + ACD_max + ACD_sum + FOUT_avg + FOUT_max + FOUT_sum + MLOC_avg + MLOC_max + MLOC_sum + NBD_avg + NBD_max + NBD_sum + NOCU + NOF_avg + NOF_max + NOF_sum + NOI_avg + NOI_max + NOI_sum + NOM_avg + NOM_max + NOM_sum + NOT_avg + NOT_max + NOT_sum + NSF_avg + NSF_max + NSF_sum + NSM_avg + NSM_max + NSM_sum + PAR_avg + PAR_max + PAR_sum + TLOC_avg + TLOC_max + TLOC_sum + VG_avg + VG_max + VG_sum, data=train, family = \"binomial\")\n",
    "    test.prob <- predict(model.glm, test, type=\"response\")\n",
    "    test.pred <- test.prob>=0.50\n",
    "\n",
    "    outcome <- table(factor(test$post>0, levels=c(F,T)), factor(test.pred, levels=c(F,T)))\n",
    "    TN <- outcome[1,1]\n",
    "    FN <- outcome[2,1]\n",
    "    FP <- outcome[1,2]\n",
    "    TP <- outcome[2,2]\n",
    "    precision <- if (TP + FP ==0) { 1 } else { TP / (TP + FP) }\n",
    "    recall <- TP / (TP + FN)\n",
    "    accuracy <- (TP + TN) / (TN + FN + FP + TP)\n",
    "    defects <- (TP + FN) / (TN + FN + FP + TP)\n",
    "    return (c(defects, precision, recall, accuracy))\n",
    "}\n",
    "\n",
    "test_classification(files_20, files_20)\n",
    "test_classification(files_20, files_21)\n",
    "test_classification(files_20, files_30)\n",
    "test_classification(files_21, files_20)\n",
    "test_classification(files_21, files_21)\n",
    "test_classification(files_21, files_30)\n",
    "test_classification(files_30, files_20)\n",
    "test_classification(files_30, files_21)\n",
    "test_classification(files_30, files_30)\n",
    "\n",
    "test_classification_pkg(packages_20, packages_20)\n",
    "test_classification_pkg(packages_20, packages_21)\n",
    "test_classification_pkg(packages_20, packages_30)\n",
    "test_classification_pkg(packages_21, packages_20)\n",
    "test_classification_pkg(packages_21, packages_21)\n",
    "test_classification_pkg(packages_21, packages_30)\n",
    "test_classification_pkg(packages_30, packages_20)\n",
    "test_classification_pkg(packages_30, packages_21)\n",
    "test_classification_pkg(packages_30, packages_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_ranking <- function (train, test) \n",
    "{\n",
    "    model.lm <- lm(post ~ pre + ACD + FOUT_avg + FOUT_max + FOUT_sum + MLOC_avg + MLOC_max + MLOC_sum + NBD_avg + NBD_max + NBD_sum + NOF_avg + NOF_max + NOF_sum + NOI + NOM_avg + NOM_max + NOM_sum + NOT + NSF_avg + NSF_max + NSF_sum + NSM_avg + NSM_max + NSM_sum + PAR_avg + PAR_max + PAR_sum + + + TLOC + VG_avg + VG_max + VG_sum, data=train)\n",
    "    test.pred <- predict(model.lm, test)\n",
    "\n",
    "    r.squared <- summary(model.lm)$r.squared\n",
    "    pearson <- cor(test$post, test.pred, method=\"pearson\")\n",
    "    spearman <- cor(test$post, test.pred, method=\"spearman\")\n",
    "    pearson.p <- cor.test(test$post, test.pred, method=\"pearson\")$p.value\n",
    "    spearman.p <- cor.test(test$post, test.pred, method=\"spearman\", exact=FALSE)$p.value\n",
    "\n",
    "    return (c(r.squared, pearson, spearman, pearson.p<0.01, spearman.p<0.01))\n",
    "}\n",
    "\n",
    "test_ranking_pkg <- function (train, test) \n",
    "{\n",
    "    model.lm <- lm(post ~ pre + ACD_avg + ACD_max + ACD_sum + FOUT_avg + FOUT_max + FOUT_sum + MLOC_avg + MLOC_max + MLOC_sum + NBD_avg + NBD_max + NBD_sum + NOCU + NOF_avg + NOF_max + NOF_sum + NOI_avg + NOI_max + NOI_sum + NOM_avg + NOM_max + NOM_sum + NOT_avg + NOT_max + NOT_sum + NSF_avg + NSF_max + NSF_sum + NSM_avg + NSM_max + NSM_sum + PAR_avg + PAR_max + PAR_sum + TLOC_avg + TLOC_max + TLOC_sum + VG_avg + VG_max + VG_sum, data=train)\n",
    "    test.pred <- predict(model.lm, test)\n",
    "\n",
    "    r.squared <- summary(model.lm)$r.squared\n",
    "    pearson <- cor(test$post, test.pred, method=\"pearson\")\n",
    "    spearman <- cor(test$post, test.pred, method=\"spearman\")\n",
    "    pearson.p <- cor.test(test$post, test.pred, method=\"pearson\")$p.value\n",
    "    spearman.p <- cor.test(test$post, test.pred, method=\"spearman\", exact=FALSE)$p.value\n",
    "\n",
    "    return (c(r.squared, pearson, spearman, pearson.p<0.01, spearman.p<0.01))\n",
    "}\n",
    "\n",
    "\n",
    "test_ranking(files_20, files_20)\n",
    "test_ranking(files_20, files_21)\n",
    "test_ranking(files_20, files_30)\n",
    "test_ranking(files_21, files_20)\n",
    "test_ranking(files_21, files_21)\n",
    "test_ranking(files_21, files_30)\n",
    "test_ranking(files_30, files_20)\n",
    "test_ranking(files_30, files_21)\n",
    "test_ranking(files_30, files_30)\n",
    "\n",
    "test_ranking_pkg(packages_20, packages_20)\n",
    "test_ranking_pkg(packages_20, packages_21)\n",
    "test_ranking_pkg(packages_20, packages_30)\n",
    "test_ranking_pkg(packages_21, packages_20)\n",
    "test_ranking_pkg(packages_21, packages_21)\n",
    "test_ranking_pkg(packages_21, packages_30)\n",
    "test_ranking_pkg(packages_30, packages_20)\n",
    "test_ranking_pkg(packages_30, packages_21)\n",
    "test_ranking_pkg(packages_30, packages_30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
